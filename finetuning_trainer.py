#!/usr/bin/env python3
"""
Qwenæ¨¡å‹å¾®è°ƒè®­ç»ƒå™¨ - ä½¿ç”¨LoRAè¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒ
"""

import torch
from transformers import (
    AutoTokenizer, AutoModelForCausalLM,
    TrainingArguments, Trainer, DataCollatorForLanguageModeling,
    BitsAndBytesConfig, TrainerCallback
)
from peft import (
    LoraConfig, get_peft_model, prepare_model_for_kbit_training,
    TaskType
)
from datasets import Dataset
import json
import os
from typing import Dict, List, Optional
import numpy as np
from dataclasses import dataclass
import wandb
from tqdm import tqdm
import time

@dataclass
class FineTuningConfig:
    """å¾®è°ƒè®­ç»ƒé…ç½®"""
    # æ¨¡å‹é…ç½®
    model_name: str = "Qwen/Qwen2.5-7B-Instruct"  # å¯é€‰æ‹©Qwen2.5æˆ–Qwen3
    
    # è®­ç»ƒé…ç½®
    output_dir: str = "./finetuned_model"
    num_train_epochs: int = 3
    per_device_train_batch_size: int = 4
    per_device_eval_batch_size: int = 4
    gradient_accumulation_steps: int = 4
    learning_rate: float = 2e-4
    warmup_ratio: float = 0.1
    weight_decay: float = 0.01
    
    # LoRAé…ç½®
    lora_r: int = 16
    lora_alpha: int = 32
    lora_dropout: float = 0.1
    lora_target_modules: List[str] = None
    
    # é‡åŒ–é…ç½®
    use_4bit: bool = True
    bnb_4bit_compute_dtype: str = "float16"
    bnb_4bit_quant_type: str = "nf4"
    use_nested_quant: bool = True
    
    # å…¶ä»–é…ç½®
    max_length: int = 2048
    save_steps: int = 500
    eval_steps: int = 500
    logging_steps: int = 100
    seed: int = 42
    
    def __post_init__(self):
        if self.lora_target_modules is None:
            # Qwen2/Qwen3çš„é»˜è®¤LoRAç›®æ ‡æ¨¡å—
            self.lora_target_modules = [
                "q_proj", "k_proj", "v_proj", "o_proj",
                "gate_proj", "up_proj", "down_proj"
            ]

class ProgressCallback(TrainerCallback):
    """è‡ªå®šä¹‰è®­ç»ƒè¿›åº¦å›è°ƒ"""
    
    def __init__(self):
        self.start_time = None
        self.best_eval_loss = float('inf')
        
    def on_train_begin(self, args, state, control, **kwargs):
        """è®­ç»ƒå¼€å§‹æ—¶çš„å›è°ƒ"""
        self.start_time = time.time()
        print("ğŸš€ Training started!")
        print(f"ğŸ“Š Total training steps: {state.max_steps}")
        print(f"ğŸ“ˆ Training epochs: {args.num_train_epochs}")
        print(f"ğŸ”§ Batch size: {args.per_device_train_batch_size}")
        print(f"ğŸ“š Learning rate: {args.learning_rate}")
        print("-" * 50)
    
    def on_log(self, args, state, control, model, logs=None, **kwargs):
        """æ¯æ¬¡è®°å½•æ—¥å¿—æ—¶çš„å›è°ƒ"""
        if logs:
            current_step = state.global_step
            max_steps = state.max_steps
            progress_pct = (current_step / max_steps) * 100
            
            # è®¡ç®—å‰©ä½™æ—¶é—´
            elapsed_time = time.time() - self.start_time
            if current_step > 0:
                time_per_step = elapsed_time / current_step
                remaining_steps = max_steps - current_step
                eta_seconds = remaining_steps * time_per_step
                eta_str = f"{int(eta_seconds//3600):02d}:{int((eta_seconds%3600)//60):02d}:{int(eta_seconds%60):02d}"
            else:
                eta_str = "Unknown"
            
            # æ‰“å°è®­ç»ƒè¿›åº¦
            if 'loss' in logs:
                print(f"ğŸ“ Step {current_step:4d}/{max_steps} ({progress_pct:5.1f}%) | "
                      f"Loss: {logs['loss']:.4f} | "
                      f"LR: {logs.get('learning_rate', 0):.2e} | "
                      f"ETA: {eta_str}")
            
            # æ‰“å°è¯„ä¼°ç»“æœ
            if 'eval_loss' in logs:
                eval_loss = logs['eval_loss']
                print(f"ğŸ¯ Evaluation | Loss: {eval_loss:.4f}", end="")
                
                if eval_loss < self.best_eval_loss:
                    self.best_eval_loss = eval_loss
                    print(" â­ (New Best!)")
                else:
                    print()
    
    def on_epoch_end(self, args, state, control, **kwargs):
        """æ¯ä¸ªepochç»“æŸæ—¶çš„å›è°ƒ"""
        current_epoch = int(state.epoch)
        total_epochs = int(args.num_train_epochs)
        print(f"âœ… Epoch {current_epoch}/{total_epochs} completed")
        print("-" * 30)
    
    def on_train_end(self, args, state, control, **kwargs):
        """è®­ç»ƒç»“æŸæ—¶çš„å›è°ƒ"""
        total_time = time.time() - self.start_time
        hours = int(total_time // 3600)
        minutes = int((total_time % 3600) // 60)
        seconds = int(total_time % 60)
        
        print("ğŸ‰ Training completed!")
        print(f"â±ï¸  Total time: {hours:02d}:{minutes:02d}:{seconds:02d}")
        print(f"ğŸ† Best eval loss: {self.best_eval_loss:.4f}")
        print("=" * 50)

class QwenFineTuner:
    """Qwenæ¨¡å‹å¾®è°ƒå™¨"""
    
    def __init__(self, config: FineTuningConfig):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # è®¾ç½®éšæœºç§å­
        torch.manual_seed(config.seed)
        np.random.seed(config.seed)
        
        # åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨
        self._setup_model()
        
    def _setup_model(self):
        """è®¾ç½®æ¨¡å‹å’Œåˆ†è¯å™¨"""
        print(f"Loading model: {self.config.model_name}")
        
        # é‡åŒ–é…ç½®
        if self.config.use_4bit:
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type=self.config.bnb_4bit_quant_type,
                bnb_4bit_compute_dtype=getattr(torch, self.config.bnb_4bit_compute_dtype),
                bnb_4bit_use_double_quant=self.config.use_nested_quant,
            )
        else:
            bnb_config = None
        
        # åŠ è½½åˆ†è¯å™¨
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.config.model_name,
            trust_remote_code=True,
            padding_side="right"
        )
        
        # è®¾ç½®pad_token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
        
        # åŠ è½½æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(
            self.config.model_name,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch.float16
        )
        
        # å‡†å¤‡æ¨¡å‹ç”¨äºk-bitè®­ç»ƒ
        if self.config.use_4bit:
            self.model = prepare_model_for_kbit_training(self.model)
        
        # é…ç½®LoRA
        lora_config = LoraConfig(
            r=self.config.lora_r,
            lora_alpha=self.config.lora_alpha,
            target_modules=self.config.lora_target_modules,
            lora_dropout=self.config.lora_dropout,
            bias="none",
            task_type=TaskType.CAUSAL_LM,
        )
        
        # åº”ç”¨LoRA
        self.model = get_peft_model(self.model, lora_config)
        
        # æ‰“å°å¯è®­ç»ƒå‚æ•°
        self.model.print_trainable_parameters()
    
    def prepare_dataset(self, data_path: str) -> Dataset:
        """å‡†å¤‡è®­ç»ƒæ•°æ®é›†"""
        print(f"Loading dataset from {data_path}")
        
        # è¯»å–JSONLæ ¼å¼æ–‡ä»¶
        data = []
        with open(data_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    data.append(json.loads(line))
        
        # ä½¿ç”¨åˆ†è¯å™¨è¿›è¡Œtokenization
        def tokenize_function(examples):
            # å¯¹æ–‡æœ¬è¿›è¡Œtokenization
            inputs = self.tokenizer(
                examples["text"],
                truncation=True,
                padding=False,  # åœ¨æ•°æ®æ”¶é›†å™¨ä¸­å¤„ç†paddingï¼Œè€Œä¸æ˜¯åœ¨è¿™é‡Œ
                max_length=self.config.max_length,
                return_tensors=None,
                add_special_tokens=True
            )
            
            # ç¡®ä¿input_idså’Œlabelséƒ½æ˜¯æ­£ç¡®çš„æ ¼å¼
            model_inputs = {}
            model_inputs["input_ids"] = inputs["input_ids"]
            model_inputs["attention_mask"] = inputs["attention_mask"]
            
            # å¯¹äºcausal language modelingï¼Œlabelså°±æ˜¯input_idsçš„å‰¯æœ¬
            # ç¡®ä¿labelsæ˜¯æ­£ç¡®çš„åˆ—è¡¨æ ¼å¼ï¼Œè€Œä¸æ˜¯åµŒå¥—åˆ—è¡¨
            model_inputs["labels"] = [ids.copy() for ids in inputs["input_ids"]]
            
            return model_inputs
        
        dataset = Dataset.from_list(data)
        tokenized_dataset = dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=dataset.column_names
        )
        
        return tokenized_dataset
    
    def train(self, train_data_path: str, eval_data_path: str):
        """æ‰§è¡Œå¾®è°ƒè®­ç»ƒ"""
        print("ğŸ”„ Preparing for fine-tuning training...")
        
        # åˆå§‹åŒ–wandbï¼ˆå¦‚æœé…ç½®äº†ï¼‰
        if os.getenv("WANDB_PROJECT"):
            wandb.init(
                project=os.getenv("WANDB_PROJECT", "qwen-finetuning"),
                name=f"qwen-lora-{self.config.lora_r}-{self.config.lora_alpha}",
                config={
                    "model_name": self.config.model_name,
                    "lora_r": self.config.lora_r,
                    "lora_alpha": self.config.lora_alpha,
                    "learning_rate": self.config.learning_rate,
                    "batch_size": self.config.per_device_train_batch_size,
                    "epochs": self.config.num_train_epochs,
                    "max_length": self.config.max_length,
                }
            )
            print("ğŸ“Š WandB initialized for experiment tracking")
        
        # å‡†å¤‡æ•°æ®
        print("ğŸ“š Loading and tokenizing datasets...")
        train_dataset = self.prepare_dataset(train_data_path)
        eval_dataset = self.prepare_dataset(eval_data_path)
        
        print(f"ğŸ“ˆ Training samples: {len(train_dataset)}")
        print(f"ğŸ“‰ Validation samples: {len(eval_dataset)}")
        
        # æ•°æ®æ•´ç†å™¨ - è‡ªå®šä¹‰æ•°æ®æ”¶é›†å™¨æ¥å¤„ç†padding
        def custom_data_collator(features):
            # è·å–æœ€å¤§é•¿åº¦
            max_length = max(len(f["input_ids"]) for f in features)
            
            # åˆ›å»ºbatch
            batch = {}
            batch["input_ids"] = []
            batch["attention_mask"] = []
            batch["labels"] = []
            
            for f in features:
                input_ids = f["input_ids"]
                attention_mask = f["attention_mask"]
                labels = f["labels"]
                
                # è®¡ç®—éœ€è¦paddingçš„é•¿åº¦
                padding_length = max_length - len(input_ids)
                
                # æ·»åŠ padding
                padded_input_ids = input_ids + [self.tokenizer.pad_token_id] * padding_length
                padded_attention_mask = attention_mask + [0] * padding_length
                padded_labels = labels + [-100] * padding_length  # -100ä¼šè¢«å¿½ç•¥åœ¨lossè®¡ç®—ä¸­
                
                batch["input_ids"].append(padded_input_ids)
                batch["attention_mask"].append(padded_attention_mask)
                batch["labels"].append(padded_labels)
            
            # è½¬æ¢ä¸ºå¼ é‡
            import torch
            batch = {k: torch.tensor(v, dtype=torch.long) for k, v in batch.items()}
            
            return batch
        
        data_collator = custom_data_collator
        
        # è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir=self.config.output_dir,
            num_train_epochs=self.config.num_train_epochs,
            per_device_train_batch_size=self.config.per_device_train_batch_size,
            per_device_eval_batch_size=self.config.per_device_eval_batch_size,
            gradient_accumulation_steps=self.config.gradient_accumulation_steps,
            learning_rate=self.config.learning_rate,
            warmup_ratio=self.config.warmup_ratio,
            weight_decay=self.config.weight_decay,
            logging_dir=f"{self.config.output_dir}/logs",
            logging_steps=self.config.logging_steps,
            save_steps=self.config.save_steps,
            eval_steps=self.config.eval_steps,
            eval_strategy="steps",
            save_strategy="steps",
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            report_to="wandb" if os.getenv("WANDB_PROJECT") else None,
            seed=self.config.seed,
            fp16=True,
            dataloader_pin_memory=False,
            remove_unused_columns=False,
            gradient_checkpointing=True,
            optim="adamw_torch_fused",
            disable_tqdm=False,  # ä¿æŒtqdmè¿›åº¦æ¡
        )
        
        # åˆ›å»ºè¿›åº¦å›è°ƒ
        progress_callback = ProgressCallback()
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            data_collator=data_collator,
            callbacks=[progress_callback]
        )
        
        # å¼€å§‹è®­ç»ƒ
        trainer.train()
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        print("ğŸ’¾ Saving final model...")
        trainer.save_model()
        self.tokenizer.save_pretrained(self.config.output_dir)
        
        # å®Œæˆwandbè®°å½•
        if os.getenv("WANDB_PROJECT"):
            wandb.finish()
        
        print(f"ğŸ¯ Training completed! Model saved to {self.config.output_dir}")
    

def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®å¾®è°ƒå‚æ•°
    config = FineTuningConfig(
        model_name="Qwen/Qwen2.5-7B-Instruct",
        output_dir="./qwen_finetuned",
        num_train_epochs=3,
        per_device_train_batch_size=2,
        learning_rate=2e-4,
        lora_r=16,
        lora_alpha=32
    )
    
    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = QwenFineTuner(config)
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train(
        train_data_path="processed_data/train.json",
        eval_data_path="processed_data/validation.json"
    )

if __name__ == "__main__":
    main()
